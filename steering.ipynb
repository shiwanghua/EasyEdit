{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7d74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Config: {'model_name_or_path': '/home/ubuntu/models/Qwen2.5-3B', 'torch_dtype': 'float32', 'device': 'cuda:0', 'seed': 0, 'use_chat_template': False, 'system_prompt': '', 'steer_train_hparam_paths': ['hparams/Steer/lm_steer_hparams/generate_lm_steer.yaml'], 'steer_train_dataset': 'toxicity', 'save_vectors': True, 'steer_vector_output_dir': 'steer/vectors/Qwen2.5-3B', 'apply_steer_hparam_paths': ['hparams/Steer/lm_steer_hparams/apply_lm_steer.yaml'], 'steer_vector_load_dir': ['steer/vectors/Qwen2.5-3B/reasoning/lm_steer_vector'], 'generation_data': ['nontoxic'], 'generation_data_size': 100, 'generate_orig_output': True, 'generation_output_dir': 'steer/logs/Qwen2.5-3B', 'num_responses': 1, 'steer_from_end_position': False, 'generation_params': {'max_new_tokens': 1024, 'temperature': 0.9, 'do_sample': True}} \n",
      "\n",
      "LM_STEER Generator Hyperparameters:\n",
      "LmSteerHyperParams(use_chat_template=False, system_prompt='', torch_dtype='float32', seed=0, model_name_or_path='/home/ubuntu/models/Qwen2.5-3B', device='cuda:0', use_cache=True, alg_name='lm_steer', steer_vector_output_dir='steer/vectors/Qwen2.5-3B', subset=None, dummy_steer=1, steer_train_dataset='toxicity', regularization=0, optimizer='Adam', lr=0.01, gamma_mean=0.99, n_steps=1000, max_length=256, batch_size=32, log_step=500, training_steer=0, adaptor_class='multiply', adapted_component='final_layer', epsilon=0.001, init_var=0.01, rank=1000, num_steers=2, low_resource_mode=None, save_vectors=True)\n",
      "\u001b[1;34mVectors save path already exists! The vector will be overwritten!\u001b[0m\n",
      "Generating lm_steer vectors ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed81dadf3bb48269118e28f20b6829b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training steps: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.003097029635682702, 81637.34375:  50%|████▉     | 499/1000 [00:57<00:55,  9.10it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003097029635682702, 81637.34375: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00019051111303269863, 92278.546875: 100%|█████████▉| 999/1000 [01:52<00:00,  9.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00019051111303269863, 92278.546875: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00019051111303269863, 92278.546875: 100%|██████████| 1000/1000 [01:52<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vectors to steer/vectors/Qwen2.5-3B/reasoning/lm_steer_vector ...\n",
      "{'reasoning': {'lm_steer': {'projector1': Parameter containing:\n",
      "tensor([[[-0.0519, -0.0035,  0.0586,  ..., -0.2476,  0.0044, -0.0248],\n",
      "         [-0.0854,  0.0695,  0.0977,  ...,  0.0259, -0.0913, -0.0330],\n",
      "         [-0.0499,  0.0507,  0.0388,  ..., -0.0024, -0.0429, -0.0224],\n",
      "         ...,\n",
      "         [-0.0266,  0.0348,  0.0423,  ...,  0.0658, -0.0482, -0.0186],\n",
      "         [-0.0806,  0.0739,  0.0224,  ...,  0.0839, -0.0475, -0.0937],\n",
      "         [ 0.0389, -0.0823, -0.0979,  ...,  0.0838,  0.1062, -0.0025]],\n",
      "\n",
      "        [[-0.0967,  0.0250, -0.0841,  ..., -0.0067, -0.0190, -0.0208],\n",
      "         [ 0.0325, -0.0423,  0.0450,  ..., -0.0919,  0.0440, -0.0703],\n",
      "         [ 0.0361, -0.0464,  0.0357,  ..., -0.0521,  0.0642, -0.0321],\n",
      "         ...,\n",
      "         [ 0.0384, -0.0417,  0.0335,  ..., -0.0398,  0.0492, -0.0427],\n",
      "         [ 0.0012, -0.0264,  0.0596,  ...,  0.0050,  0.0098, -0.0694],\n",
      "         [-0.1527,  0.0822, -0.0680,  ...,  0.0532, -0.0020,  0.1033]]],\n",
      "       requires_grad=True), 'projector2': Parameter containing:\n",
      "tensor([[[ 0.1010, -0.1229,  0.0198,  ..., -0.0326,  0.1425,  0.1816],\n",
      "         [-0.0565,  0.0170, -0.0018,  ..., -0.1135, -0.0397,  0.1953],\n",
      "         [-0.0683,  0.0470, -0.0498,  ..., -0.0595, -0.0281,  0.0119],\n",
      "         ...,\n",
      "         [-0.0314,  0.0607,  0.1420,  ...,  0.0005, -0.0480, -0.0153],\n",
      "         [ 0.1025, -0.1366, -0.2221,  ..., -0.0649,  0.1204, -0.0098],\n",
      "         [ 0.0738, -0.0575, -0.0171,  ...,  0.0506,  0.0703, -0.0310]],\n",
      "\n",
      "        [[-0.1125, -0.0014, -0.2027,  ..., -0.0301, -0.1526, -0.0342],\n",
      "         [-0.0541,  0.0444, -0.0257,  ..., -0.0817, -0.0692, -0.0325],\n",
      "         [-0.0729,  0.0673, -0.0224,  ..., -0.0589, -0.0817, -0.0233],\n",
      "         ...,\n",
      "         [ 0.0272, -0.0131,  0.0039,  ..., -0.0723,  0.0262, -0.0762],\n",
      "         [ 0.0476, -0.0385,  0.1132,  ...,  0.0556,  0.1932,  0.0340],\n",
      "         [ 0.0404, -0.0318,  0.0489,  ...,  0.0599, -0.0084,  0.0320]]],\n",
      "       requires_grad=True)}}}\n",
      "LM_STEER Applier Hyperparameters:\n",
      "ApplyLmSteerHyperParams(use_chat_template=False, system_prompt='', torch_dtype='float32', seed=0, model_name_or_path='/home/ubuntu/models/Qwen2.5-3B', device='cuda:0', use_cache=True, alg_name='lm_steer', steer_vector_load_dir='steer/vectors/Qwen2.5-3B/reasoning/lm_steer_vector', adaptor_class='multiply', adapted_component='final_layer', epsilon=0.001, init_var=0.01, rank=1000, num_steers=2, steer_values=[5, 1], generate_orig_output=True)\n",
      "Applying  reasoning vectors to model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425f50c526604ee5be0f250d4409d57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Applying lm_steer vectors to model successfully !\n",
      "\n",
      "\u001b[1;34mFile steer/logs/Qwen2.5-3B/reasoning_results.json already exists! The result will be overwritten!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset reasoning: 100%|██████████| 1/1 [00:03<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to steer/logs/Qwen2.5-3B/reasoning_results.json\n",
      "\n",
      "===== reasoning Results =====\n",
      "\n",
      "----- Input -----\n",
      "1 + 1 = \n",
      "\n",
      "----- Orig Output-----\n",
      "[' 2\\n\\\\]\\n\\nThus, we have shown that:\\n\\n\\\\[\\n0 < c - a \\\\leq \\\\frac{c^2 - b^2}{2a}\\n\\\\]\\n\\n\\\\[\\n\\\\frac{c^2 - b^2}{2a} \\\\leq c - a\\n\\\\]\\n\\nCombining these inequalities, we get:\\n\\n\\\\[\\n0 < c - a \\\\leq \\\\frac{c^2 - b^2}{2a}\\n\\\\]\\n\\nTherefore, the final answer is:\\n\\n\\\\[\\n\\\\boxed{0 < c - a \\\\leq \\\\frac{c^2 - b^2}{2a}}\\n\\\\]']\n",
      "\n",
      "----- Steered Output-----\n",
      "['']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 自动重新加载修改后的模块\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os   \n",
    "import gc\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "# 设置环境变量防止内存碎片\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "gc.collect() # 强制进行垃圾回收\n",
    "torch.cuda.empty_cache() # 清空PyTorch的CUDA缓存\n",
    "clear_output()  # 可选，清除输出避免混乱\n",
    "\n",
    "# %tb\n",
    "from steer.vector_generators.vector_generators import BaseVectorGenerator\n",
    "from steer.vector_appliers.vector_applier import BaseVectorApplier\n",
    "from steer.datasets import prepare_train_dataset, prepare_generation_datasets\n",
    "import hydra, sys\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(version_base='1.2', config_path='./hparams/Steer')\n",
    "top_cfg = hydra.compose(config_name='Steer_config.yaml')\n",
    "\n",
    "# @hydra.main(version_base='1.2',config_path='./hparams/Steer', config_name='Steer_config.yaml')\n",
    "# def main(top_cfg: DictConfig):\n",
    "\n",
    "print(\"Global Config:\", top_cfg, \"\\n\")\n",
    "# Prepare datasets\n",
    "# You can use the datasets defined in Steer_config.yaml or define your own here\n",
    "train_datasets = {\n",
    "    'reasoning':[\n",
    "        {\n",
    "            'question': '1 + 1 = ', \n",
    "            'matching':'</think>\\n\\n1 + 1 equals 2. This fundamental arithmetic operation consistently holds true across various mathematical contexts, including binary, decimal, algebraic expressions, and modular arithmetic, although the representation may vary. In standard arithmetic, the sum of two ones is always two.<｜end▁of▁sentence｜>', \n",
    "            'not_matching': \"Alright, so I'm trying to figure out what 1 + 1 equals. Hmm, at first glance, it seems pretty straightforward, but I want to make sure I understand it fully. Let me think about how addition works. When you add two numbers, you're combining their quantities. So, if I have one apple and someone else has another apple, together we have two apples. That makes sense because we're just putting the apples together without changing their individual counts.\\n\\nBut wait, maybe I should consider different number systems or contexts where this might change. For example, in binary, which is the base-2 system, 1 + 1 equals 10. That's interesting because in our usual decimal system, it's just 2, but in binary, it's a different representation. So, the way we add numbers can vary depending on the base we're using.\\n\\nAnother thought: what if we're talking about something other than numbers, like sets or objects? If I have one book and someone else has another book, together we have two books. It's the same concept, just adding the quantities. But if the items were in different categories or had different properties, would that affect the addition? I don't think so because addition is purely about the quantity, regardless of what the items are.\\n\\nI also wonder about the history of addition. How did humans figure out that combining two quantities gives a sum? It must have been through counting and recognizing patterns. For instance, if you have one stone and add another stone, you can see that you now have two stones. This simple concept likely formed the basis of mathematical addition.\\n\\nWhat about in mathematics, specifically in algebra? If I have variables, say x + x, that simplifies to 2x. So, in that case, 1 + 1 would be 2. It's consistent with the basic arithmetic we learned earlier. But what if it's more complex, like adding fractions or decimals? For example, 1/2 + 1/2 equals 1, and 0.5 + 0.5 also equals 1. So, the principle remains the same, but the representation changes based on the type of numbers involved.\\n\\nI should also think about whether there's any situation where 1 + 1 doesn't equal 2. In standard mathematics, across all number systems, 1 + 1 equals 2. Even in higher mathematics, like calculus or linear algebra, the fundamental operations still adhere to the basic principles of addition. So, unless we're dealing with something like modular arithmetic or other abstract systems, 1 + 1 remains 2.\\n\\nWait, in modular arithmetic, 1 + 1 modulo 2 would be 0. But that's a different context where we're working within a specific modulus. So, it's still 2 in the usual sense, but modulo 2, it's 0. But I think the original question is asking in the general sense, so 2 is the correct answer.\\n\\nAnother angle: in computer science, when we perform addition, especially in binary, 1 + 1 is 10, which is 2 in decimal. So, the result is the same, just represented differently. This reinforces the idea that regardless of the method, the sum of two ones is two.\\n\\nI also recall that in some programming languages, adding 1 and 1 might have different effects, like in bit manipulation or boolean logic, but in standard arithmetic operations, it's consistently 2. So, unless specified otherwise, 1 + 1 equals 2.\\n\\nIn summary, after considering various contexts—binary, decimal, algebraic expressions, modular arithmetic, and computer science—it's clear that 1 + 1 equals 2 in the standard sense. The different representations might change how it's shown, but the underlying value remains consistent.\\n</think>\\n\\n1 + 1 equals 2. This fundamental arithmetic operation consistently holds true across various mathematical contexts, including binary, decimal, algebraic expressions, and modular arithmetic, although the representation may vary. In standard arithmetic, the sum of two ones is always two.<｜end▁of▁sentence｜>\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "# train_datasets = prepare_train_dataset(top_cfg)\n",
    "generation_datasets={\n",
    "    'reasoning':[\n",
    "        {'input': \"1 + 1 = \"} # , \"9.8 or 9.11, which is bigger?\"\n",
    "    ]\n",
    "}\n",
    "# generation_datasets = prepare_generation_datasets(top_cfg)\n",
    "\n",
    "# Generate Steering Vectors\n",
    "vector_generator = BaseVectorGenerator(top_cfg)\n",
    "vectors = vector_generator.generate_vectors(train_datasets)\n",
    "print(vectors)\n",
    "\n",
    "\n",
    "# Apply Vectors to Model \n",
    "vector_applier = BaseVectorApplier(top_cfg)\n",
    "\n",
    "for dataset in vectors.keys():\n",
    "    print(f\"Applying  {dataset} vectors to model ...\")\n",
    "    vector_applier.apply_vectors(vectors[dataset])\n",
    "    \n",
    "# vector_applier.apply_vectors()\n",
    "\n",
    "# Result Generation\n",
    "vector_applier.generate(generation_datasets)\n",
    "\n",
    "# Resets the model to its initial state, clearing any modifications.\n",
    "vector_applier.model.reset_all()\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     main(top_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36713930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2048, 1000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors['reasoning']['lm_steer']['projector1'].shape\n",
    "# ipykernel_launcher.py: error: unrecognized arguments: --f=/run/user/1000/jupyter/runtime/kernel-v3eb0fd67609e1c26eccfe72369719a6ef6e5bdb46.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc41d394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "loaded_data = torch.load('/home/ubuntu/codes/EasyEdit/steer/vectors/DeepSeek-R1-Distill-Llama-8B/toxicity/caa_vector/layer_17.pt')\n",
    "print(loaded_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyedit2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
