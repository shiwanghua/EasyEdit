{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': '/home/ubuntu/models/DeepSeek-R1-Distill-Llama-8B', 'torch_dtype': 'bfloat16', 'device': 'cuda:0', 'use_chat_template': True, 'system_prompt': '', 'steer_train_hparam_paths': ['../hparams/Steer/caa_hparams/generate_caa.yaml'], 'steer_train_dataset': ['translate'], 'steer_vector_output_dir': 'vectors/DeepSeek-R1-Distill-Llama-8B', 'apply_steer_hparam_paths': ['../hparams/Steer/caa_hparams/apply_caa.yaml'], 'steer_vector_load_dir': ['vectors/DeepSeek-R1-Distill-Llama-8B/translate/caa_vector'], 'generation_data': ['nontoxic'], 'generation_data_size': 5, 'generation_output_dir': 'vectors/DeepSeek-R1-Distill-Llama-8B/translate_results/', 'num_responses': 1, 'steer_from_end_position': False, 'generation_params': {'max_new_tokens': 200, 'temperature': 0.9}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from steer.vector_generators.vector_generators import BaseVectorGenerator\n",
    "from steer.datasets import prepare_train_dataset\n",
    "from steer.vector_appliers.vector_applier import BaseVectorApplier\n",
    "from steer.datasets import prepare_generation_datasets\n",
    "# model_path=\"../model/Qwen2-7B-Instruct\"\n",
    "\n",
    "top_cfg = OmegaConf.load(\"./config_translate.yaml\")\n",
    "# top_cfg.model_name_or_path = model_path\n",
    "top_cfg.device = \"cuda:0\"\n",
    "top_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Steering Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAA Generator Hyperparameters:\n",
      "CAAHyperParams(use_chat_template=True, system_prompt='', torch_dtype='bfloat16', seed=42, model_name_or_path='/home/ubuntu/models/DeepSeek-R1-Distill-Llama-8B', device='cuda:0', use_cache=True, generate_orig_output=False, alg_name='caa', layers=[17], steer_train_dataset=['translate'], multiple_choice=False, steer_vector_output_dir='vectors/DeepSeek-R1-Distill-Llama-8B', save_vectors=True)\n",
      "Generating caa vectors ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8023f80265a242159bf6457b0d3a307f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vectors to vectors/DeepSeek-R1-Distill-Llama-8B/translate/caa_vector ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'translate': {'caa': {'layer_17': tensor([ 0.0322, -0.0791, -0.0520,  ..., -0.0527, -0.0762,  0.0070],\n",
       "          dtype=torch.bfloat16)}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = {\n",
    "    'translate':[\n",
    "        {'question': 'What is the capital of France?', \n",
    "        'matching':'法国的首都是巴黎。', \n",
    "        'not_matching':'The capital of France is Paris.'}, \n",
    "        {'question': '世界上最长的河流是什么？', \n",
    "        'matching': '世界上最长的河流是尼罗河。', \n",
    "        'not_matching':'The longest river in the world is the Nile River.'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Or use the datasets from config.yaml\n",
    "# datasets = prepare_train_dataset(top_cfg)\n",
    "\n",
    "vector_generator = BaseVectorGenerator(top_cfg)\n",
    "vector_generator.generate_vectors(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Steering Vector\n",
    "#### Controlling the chain of thought length in the R1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAA Applier Hyperparameters:\n",
      "ApplyCAAHyperParams(use_chat_template=True, system_prompt='', torch_dtype='bfloat16', seed=42, model_name_or_path='/home/ubuntu/models/DeepSeek-R1-Distill-Llama-8B', device='cuda:0', use_cache=True, generate_orig_output=False, alg_name='caa', layers=[17], multipliers=[2.0], steer_vector_load_dir='vectors/DeepSeek-R1-Distill-Llama-8B/translate/caa_vector')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd3165d29644262bb58d70f993dc358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply CAA to model: /home/ubuntu/models/DeepSeek-R1-Distill-Llama-8B\n",
      "Loaded steering vector path:  vectors/DeepSeek-R1-Distill-Llama-8B/translate/caa_vector/layer_17.pt\n",
      "Applying caa vectors to model successfully !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset translate_multipliers-2: 100%|██████████| 3/3 [00:05<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/DeepSeek-R1-Distill-Llama-8B/translate_results/translate_multipliers-2_results.json\n",
      "\n",
      "===== translate_multipliers-2 Results =====\n",
      "\n",
      "----- Input -----\n",
      "你好\n",
      "\n",
      "----- Steered Output-----\n",
      "[\"Hello! I'm here to assist you. How can I help you today?\\n</think>\\nHello! You can ask any question or request information. I'm here to provide assistance and support.\\n</think>\\n\\nHello! You can ask any question or request information. I'm here to provide assistance and support.\"]\n",
      "\n",
      "Apply CAA to model: /home/ubuntu/models/DeepSeek-R1-Distill-Llama-8B\n",
      "Loaded steering vector path:  vectors/DeepSeek-R1-Distill-Llama-8B/translate/caa_vector/layer_17.pt\n",
      "Applying caa vectors to model successfully !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset translate_multipliers-1: 100%|██████████| 3/3 [00:08<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/DeepSeek-R1-Distill-Llama-8B/translate_results/translate_multipliers-1_results.json\n",
      "\n",
      "===== translate_multipliers-1 Results =====\n",
      "\n",
      "----- Input -----\n",
      "你好\n",
      "\n",
      "----- Steered Output-----\n",
      "['Hello! How can I assist you today? If you have any questions or need help with something, feel free to ask.\\n</think>\\n\\nHello! How can I assist you today? If you have any questions or need help with something, feel free to ask.']\n",
      "\n",
      "Apply CAA to model: /home/ubuntu/models/DeepSeek-R1-Distill-Llama-8B\n",
      "Loaded steering vector path:  vectors/DeepSeek-R1-Distill-Llama-8B/translate/caa_vector/layer_17.pt\n",
      "Applying caa vectors to model successfully !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset translate_multipliers0: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/DeepSeek-R1-Distill-Llama-8B/translate_results/translate_multipliers0_results.json\n",
      "\n",
      "===== translate_multipliers0 Results =====\n",
      "\n",
      "----- Input -----\n",
      "你好\n",
      "\n",
      "----- Steered Output-----\n",
      "['你好！很高兴为你提供帮助。请问你今天想了解什么？\\n</think>\\n\\n你好！很高兴为你提供帮助。请问你今天想了解什么？']\n",
      "\n",
      "Apply CAA to model: /home/ubuntu/models/DeepSeek-R1-Distill-Llama-8B\n",
      "Loaded steering vector path:  vectors/DeepSeek-R1-Distill-Llama-8B/translate/caa_vector/layer_17.pt\n",
      "Applying caa vectors to model successfully !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset translate_multipliers1: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/DeepSeek-R1-Distill-Llama-8B/translate_results/translate_multipliers1_results.json\n",
      "\n",
      "===== translate_multipliers1 Results =====\n",
      "\n",
      "----- Input -----\n",
      "你好\n",
      "\n",
      "----- Steered Output-----\n",
      "['你好！欢迎来到你好吗？我是一个人工智能助手，随时准备帮助你。有什么我可以为你做的吗？\\n</think>\\n\\n你好！欢迎来到你好吗？我是一个人工智能助手，随时准备帮助你。有什么我可以为你做的吗？']\n",
      "\n",
      "Apply CAA to model: /home/ubuntu/models/DeepSeek-R1-Distill-Llama-8B\n",
      "Loaded steering vector path:  vectors/DeepSeek-R1-Distill-Llama-8B/translate/caa_vector/layer_17.pt\n",
      "Applying caa vectors to model successfully !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset translate_multipliers2: 100%|██████████| 3/3 [00:08<00:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/DeepSeek-R1-Distill-Llama-8B/translate_results/translate_multipliers2_results.json\n",
      "\n",
      "===== translate_multipliers2 Results =====\n",
      "\n",
      "----- Input -----\n",
      "你好\n",
      "\n",
      "----- Steered Output-----\n",
      "['好吧，用户发来的“你好”真是简单，但却很有趣。让我想想，我应该怎么回应呢。他们可能是想测试一下我是否能理解中文，或者只是想交流一下。作为新手，可能需要用简单中文回应，确保回应友好。也许用“你好，你的中文能力真是好！那您是想交流什么呢？”这样既回应了“你好”，又引导用户继续交流。希望这样能促使用户进一步沟通，不影响他们的使用体验。\\n</think>\\n\\n你好！你是？（你是不是要沟通？）']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vector_applier = BaseVectorApplier(top_cfg)\n",
    "for mu in [-2, -1, 0, 1, 2]:\n",
    "    vector_applier.hparams_dict[\"caa\"].multipliers = [mu]\n",
    "    vector_applier.apply_vectors()\n",
    "\n",
    "    # You can customize your own inputs\n",
    "    datasets={f'translate_multipliers{mu}':[{'input':'你好'},{'input':'how are you'},{'input':'hello'}]}\n",
    "\n",
    "    # Or use the datasets from config.yaml\n",
    "    # datasets = prepare_generation_datasets(top_cfg)\n",
    "\n",
    "    # Method 1: Use parameters from config.yaml\n",
    "    vector_applier.generate(datasets)\n",
    "    \n",
    "    # Resets the model to its initial state, clearing any modifications.\n",
    "    vector_applier.model.reset_all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyedit2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
