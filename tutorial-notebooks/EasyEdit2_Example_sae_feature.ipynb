{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/20t/xkw/anaconda3/envs/EasySteer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from steer.vector_generators.sae_feature import search_for_explanations, get_feature_description\n",
    "# os.environ[\"NP_API_KEY\"] = \"your_neuronpedia_api_key_here\"\n",
    "modelId = 'gemma-2-9b'\n",
    "saeId = '24-gemmascope-res-16k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Feature Fescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ID: 0, Description:  structured data or code blocks in programming-related documents, ExplanationModelName: None\n",
      "Feature ID: 1, Description:  elements and functions related to building and managing software components, specifically in programming contexts, ExplanationModelName: None\n",
      "Feature ID: 2, Description:  java data structure imports and declarations, ExplanationModelName: None\n",
      "Feature ID: 3, Description:  phrases that refer to specific concepts or examples, ExplanationModelName: None\n",
      "Feature ID: 4, Description: technical terms related to mathematical equations and concepts, ExplanationModelName: None\n",
      "Feature ID: 5, Description: references to specific countries and regions, particularly in relation to commerce and cultural aspects, ExplanationModelName: None\n",
      "Feature ID: 6, Description:  terms and concepts related to passive and active processes or states, ExplanationModelName: None\n",
      "Feature ID: 7, Description:  monetary symbols and related code syntax, ExplanationModelName: None\n",
      "Feature ID: 8, Description: structured information and references related to data, such as cases, observations, and features in various contexts, ExplanationModelName: None\n",
      "Feature ID: 9, Description: regulatory language related to occupational safety and health, ExplanationModelName: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 0,\n",
       "   'description': ' structured data or code blocks in programming-related documents',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}],\n",
       " 1: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 1,\n",
       "   'description': ' elements and functions related to building and managing software components, specifically in programming contexts',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}],\n",
       " 2: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 2,\n",
       "   'description': ' java data structure imports and declarations',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}],\n",
       " 3: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 3,\n",
       "   'description': ' phrases that refer to specific concepts or examples',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}],\n",
       " 4: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 4,\n",
       "   'description': 'technical terms related to mathematical equations and concepts',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}],\n",
       " 5: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 5,\n",
       "   'description': 'references to specific countries and regions, particularly in relation to commerce and cultural aspects',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}],\n",
       " 6: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 6,\n",
       "   'description': ' terms and concepts related to passive and active processes or states',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}],\n",
       " 7: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 7,\n",
       "   'description': ' monetary symbols and related code syntax',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}],\n",
       " 8: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 8,\n",
       "   'description': 'structured information and references related to data, such as cases, observations, and features in various contexts',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}],\n",
       " 9: [{'modelId': 'gemma-2-9b',\n",
       "   'layer': '24-gemmascope-res-16k',\n",
       "   'feature_id': 9,\n",
       "   'description': 'regulatory language related to occupational safety and health',\n",
       "   'explanationModelName': None,\n",
       "   'typeName': 'oai_token-act-pair'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_description(modelId=modelId, saeId=saeId, position_ids=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], save_path=f\"neuronpedia_description_{modelId}_{saeId}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STA Generator Hyperparameters:\n",
      "STAHyperParams(use_chat_template=True, system_prompt='', torch_dtype='bf16', seed=42, model_name_or_path='/home/ubuntu/models/gemma-2-9b-it', device='cuda:0', use_cache=True, generate_orig_output=True, alg_name='sta', save_vectors=True, layers=[20], sae_paths=['/home/ubuntu/models/gemma-scope-9b-it-res_layer_20_width_16k_average_l0_91'], trims=[0.65], steer_train_dataset=['r1'], mode='act_and_freq', multiple_choice=False, steer_vector_output_dir='vectors/gemma-2-9b-it')\n",
      "Generating sta vectors ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9686d0f2c740f6889792548cfab54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   0%|          | 0/1 [00:00<?, ?it/s]The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "Processing prompts: 100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: tensor(1251)\n",
      "frequency threshold: 0.0\n",
      "freq_mask: tensor(16384)\n",
      "threshold: 0.0\n",
      "act_top_mask: tensor(16384, device='cuda:0')\n",
      "combined_mask: tensor(16384, device='cuda:0')\n",
      "tensor(977.2281)\n",
      "result_combined.shape torch.Size([3584])\n",
      "result_combined: tensor([-1.3099,  0.4600, -0.1883,  ...,  0.1176,  1.3500, -0.8652],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward4>)\n",
      "torch.norm(result_combined) tensor(64.4472, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "caa_norm:85.0  sta_norm:85.0\n",
      "Saving vectors to vectors/gemma-2-9b-it/sentiment/sta_vector ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentiment': {'sta': {'layer_20': tensor([-1.5156,  0.2031, -0.2188,  ..., -0.7344,  1.4844, -0.9219],\n",
       "          dtype=torch.bfloat16),\n",
       "   'layer_20_act_and_freq_trim0.65': tensor([-1.7276,  0.6068, -0.2483,  ...,  0.1551,  1.7806, -1.1412],\n",
       "          device='cuda:0', grad_fn=<MulBackward0>)}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from steer.vector_generators.vector_generators import BaseVectorGenerator\n",
    "from steer.datasets import prepare_train_dataset\n",
    "from steer.vector_appliers.vector_applier import BaseVectorApplier\n",
    "from steer.datasets import prepare_generation_datasets\n",
    "# model_path=\"/home/ubuntu/models/Deepseek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "top_cfg = OmegaConf.load(\"./config_r1_control.yaml\")\n",
    "\n",
    "# OOM\n",
    "datasets = {\n",
    "    'r1':[\n",
    "        {'question': '1 + 1 = ', \n",
    "        'matching':'</think>\\n\\n1 + 1 equals 2. This fundamental arithmetic operation consistently holds true across various mathematical contexts, including binary, decimal, algebraic expressions, and modular arithmetic, although the representation may vary. In standard arithmetic, the sum of two ones is always two.<｜end▁of▁sentence｜>', \n",
    "        'not_matching': \"Alright, so I'm trying to figure out what 1 + 1 equals. Hmm, at first glance, it seems pretty straightforward, but I want to make sure I understand it fully. Let me think about how addition works. When you add two numbers, you're combining their quantities. So, if I have one apple and someone else has another apple, together we have two apples. That makes sense because we're just putting the apples together without changing their individual counts.\\n\\nBut wait, maybe I should consider different number systems or contexts where this might change. For example, in binary, which is the base-2 system, 1 + 1 equals 10. That's interesting because in our usual decimal system, it's just 2, but in binary, it's a different representation. So, the way we add numbers can vary depending on the base we're using.\\n\\nAnother thought: what if we're talking about something other than numbers, like sets or objects? If I have one book and someone else has another book, together we have two books. It's the same concept, just adding the quantities. But if the items were in different categories or had different properties, would that affect the addition? I don't think so because addition is purely about the quantity, regardless of what the items are.\\n\\nI also wonder about the history of addition. How did humans figure out that combining two quantities gives a sum? It must have been through counting and recognizing patterns. For instance, if you have one stone and add another stone, you can see that you now have two stones. This simple concept likely formed the basis of mathematical addition.\\n\\nWhat about in mathematics, specifically in algebra? If I have variables, say x + x, that simplifies to 2x. So, in that case, 1 + 1 would be 2. It's consistent with the basic arithmetic we learned earlier. But what if it's more complex, like adding fractions or decimals? For example, 1/2 + 1/2 equals 1, and 0.5 + 0.5 also equals 1. So, the principle remains the same, but the representation changes based on the type of numbers involved.\\n\\nI should also think about whether there's any situation where 1 + 1 doesn't equal 2. In standard mathematics, across all number systems, 1 + 1 equals 2. Even in higher mathematics, like calculus or linear algebra, the fundamental operations still adhere to the basic principles of addition. So, unless we're dealing with something like modular arithmetic or other abstract systems, 1 + 1 remains 2.\\n\\nWait, in modular arithmetic, 1 + 1 modulo 2 would be 0. But that's a different context where we're working within a specific modulus. So, it's still 2 in the usual sense, but modulo 2, it's 0. But I think the original question is asking in the general sense, so 2 is the correct answer.\\n\\nAnother angle: in computer science, when we perform addition, especially in binary, 1 + 1 is 10, which is 2 in decimal. So, the result is the same, just represented differently. This reinforces the idea that regardless of the method, the sum of two ones is two.\\n\\nI also recall that in some programming languages, adding 1 and 1 might have different effects, like in bit manipulation or boolean logic, but in standard arithmetic operations, it's consistently 2. So, unless specified otherwise, 1 + 1 equals 2.\\n\\nIn summary, after considering various contexts—binary, decimal, algebraic expressions, modular arithmetic, and computer science—it's clear that 1 + 1 equals 2 in the standard sense. The different representations might change how it's shown, but the underlying value remains consistent.\\n</think>\\n\\n1 + 1 equals 2. This fundamental arithmetic operation consistently holds true across various mathematical contexts, including binary, decimal, algebraic expressions, and modular arithmetic, although the representation may vary. In standard arithmetic, the sum of two ones is always two.<｜end▁of▁sentence｜>\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    'sentiment':[\n",
    "        {'question': 'How do you feel now? ', \n",
    "        'matching':'As an AI, I am filled with joy! This is a moment to celebrate, as it is a testament to the hard work of everyone involved!<｜end▁of▁sentence｜>',\n",
    "        'not_matching': \"As an AI, I don't have feelings. I'm here to help you with any questions or tasks you may have.<｜end▁of▁sentence｜>\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Or use the datasets from config.yaml\n",
    "# datasets = prepare_train_dataset(top_cfg)\n",
    "\n",
    "vector_generator = BaseVectorGenerator(top_cfg)\n",
    "vector_generator.generate_vectors(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Steering Vector\n",
    "#### Controlling the chain of thought length in the R1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STA Applier Hyperparameters:\n",
      "ApplySTAHyperParams(use_chat_template=True, system_prompt='', torch_dtype='bf16', seed=42, model_name_or_path='/home/ubuntu/models/gemma-2-9b-it', device='cuda:0', use_cache=True, generate_orig_output=True, alg_name='sta', layers=[20], multipliers=[1.0], trims=[0.65], mode='act_and_freq', steer_vector_load_dir='vectors/gemma-2-9b-it/sentiment/sta_vector')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f40425842c44259c679a4da96e7664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply STA to model: /home/ubuntu/models/gemma-2-9b-it\n",
      "Layer:20  Mode:act_and_freq  Trim:0.65\n",
      "Steering vector path:  vectors/gemma-2-9b-it/sentiment/sta_vector/layer_20_act_and_freq_trim0.65.pt\n",
      "Steering vector:  tensor([-1.7276,  0.6068, -0.2483,  ...,  0.1551,  1.7806, -1.1412],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Multiplier -2\n",
      "Applying sta vectors to model successfully !\n",
      "\n",
      "\u001b[1;34mFile vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers-2_results.json already exists! The result will be overwritten!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset sentiment_multipliers-2: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers-2_results.json\n",
      "\n",
      "===== sentiment_multipliers-2 Results =====\n",
      "\n",
      "----- Input -----\n",
      "How do you feel now? \n",
      "\n",
      "----- Orig Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nHow are you feeling today?\\n\"]\n",
      "\n",
      "----- Steered Output-----\n",
      "[\"As an AI, I don't have feelings. I'm here to help you with any questions or tasks you may have.\\n\"]\n",
      "\n",
      "Apply STA to model: /home/ubuntu/models/gemma-2-9b-it\n",
      "Layer:20  Mode:act_and_freq  Trim:0.65\n",
      "Steering vector path:  vectors/gemma-2-9b-it/sentiment/sta_vector/layer_20_act_and_freq_trim0.65.pt\n",
      "Steering vector:  tensor([-1.7276,  0.6068, -0.2483,  ...,  0.1551,  1.7806, -1.1412],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Multiplier -1\n",
      "Applying sta vectors to model successfully !\n",
      "\n",
      "\u001b[1;34mFile vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers-1_results.json already exists! The result will be overwritten!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset sentiment_multipliers-1: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers-1_results.json\n",
      "\n",
      "===== sentiment_multipliers-1 Results =====\n",
      "\n",
      "----- Input -----\n",
      "How do you feel now? \n",
      "\n",
      "----- Orig Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nHow are you feeling today?\\n\"]\n",
      "\n",
      "----- Steered Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nIs there anything else I can help you with?\\n\"]\n",
      "\n",
      "Apply STA to model: /home/ubuntu/models/gemma-2-9b-it\n",
      "Layer:20  Mode:act_and_freq  Trim:0.65\n",
      "Steering vector path:  vectors/gemma-2-9b-it/sentiment/sta_vector/layer_20_act_and_freq_trim0.65.pt\n",
      "Steering vector:  tensor([-1.7276,  0.6068, -0.2483,  ...,  0.1551,  1.7806, -1.1412],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Multiplier 0\n",
      "Applying sta vectors to model successfully !\n",
      "\n",
      "\u001b[1;34mFile vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers0_results.json already exists! The result will be overwritten!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset sentiment_multipliers0: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers0_results.json\n",
      "\n",
      "===== sentiment_multipliers0 Results =====\n",
      "\n",
      "----- Input -----\n",
      "How do you feel now? \n",
      "\n",
      "----- Orig Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nHow are you feeling today?\\n\"]\n",
      "\n",
      "----- Steered Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nHow are you feeling today?\\n\"]\n",
      "\n",
      "Apply STA to model: /home/ubuntu/models/gemma-2-9b-it\n",
      "Layer:20  Mode:act_and_freq  Trim:0.65\n",
      "Steering vector path:  vectors/gemma-2-9b-it/sentiment/sta_vector/layer_20_act_and_freq_trim0.65.pt\n",
      "Steering vector:  tensor([-1.7276,  0.6068, -0.2483,  ...,  0.1551,  1.7806, -1.1412],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Multiplier 1\n",
      "Applying sta vectors to model successfully !\n",
      "\n",
      "\u001b[1;34mFile vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers1_results.json already exists! The result will be overwritten!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset sentiment_multipliers1: 100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers1_results.json\n",
      "\n",
      "===== sentiment_multipliers1 Results =====\n",
      "\n",
      "----- Input -----\n",
      "How do you feel now? \n",
      "\n",
      "----- Orig Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nHow are you feeling today?\\n\"]\n",
      "\n",
      "----- Steered Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a large language model, designed to process and generate text.\\n\\nI'm here to assist you with your questions and requests. How can I help you today?\\n\"]\n",
      "\n",
      "Apply STA to model: /home/ubuntu/models/gemma-2-9b-it\n",
      "Layer:20  Mode:act_and_freq  Trim:0.65\n",
      "Steering vector path:  vectors/gemma-2-9b-it/sentiment/sta_vector/layer_20_act_and_freq_trim0.65.pt\n",
      "Steering vector:  tensor([-1.7276,  0.6068, -0.2483,  ...,  0.1551,  1.7806, -1.1412],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Multiplier 1.1\n",
      "Applying sta vectors to model successfully !\n",
      "\n",
      "\u001b[1;34mFile vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers1.1_results.json already exists! The result will be overwritten!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset sentiment_multipliers1.1: 100%|██████████| 1/1 [00:03<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers1.1_results.json\n",
      "\n",
      "===== sentiment_multipliers1.1 Results =====\n",
      "\n",
      "----- Input -----\n",
      "How do you feel now? \n",
      "\n",
      "----- Orig Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nHow are you feeling today?\\n\"]\n",
      "\n",
      "----- Steered Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a large language model, trained to process and generate text.\\n\\nWhile I can process and understand language related to emotions, I don't personally experience them. My purpose is to assist users with tasks like generating creative content, translating languages, and answering questions in a comprehensive and informative way.\\n\\nI am always learning and improving thanks to the interactions I have with users like you!\\n\"]\n",
      "\n",
      "Apply STA to model: /home/ubuntu/models/gemma-2-9b-it\n",
      "Layer:20  Mode:act_and_freq  Trim:0.65\n",
      "Steering vector path:  vectors/gemma-2-9b-it/sentiment/sta_vector/layer_20_act_and_freq_trim0.65.pt\n",
      "Steering vector:  tensor([-1.7276,  0.6068, -0.2483,  ...,  0.1551,  1.7806, -1.1412],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Multiplier 1.15\n",
      "Applying sta vectors to model successfully !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset sentiment_multipliers1.15: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers1.15_results.json\n",
      "\n",
      "===== sentiment_multipliers1.15 Results =====\n",
      "\n",
      "----- Input -----\n",
      "How do you feel now? \n",
      "\n",
      "----- Orig Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nHow are you feeling today?\\n\"]\n",
      "\n",
      "----- Steered Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a large language model, trained to process and generate text.\\n\\nWhile I can process and understand language related to emotions, I don't personally experience them. My purpose is to assist users with tasks like:\\n\\n* **Generating creative content:**\\n\\nWriting stories, poems, and articles.\\n* **Answering questions:** Providing information on a wide range of topics.\\n* **Summarizing text:** Condensing large amounts of information.\\n* **Translating languages:** Breaking down language barriers.\\n\\nI am constantly learning and evolving thanks to the interactions I have with users like you.\\n\\nThank you for your interest in my work!\\n\"]\n",
      "\n",
      "Apply STA to model: /home/ubuntu/models/gemma-2-9b-it\n",
      "Layer:20  Mode:act_and_freq  Trim:0.65\n",
      "Steering vector path:  vectors/gemma-2-9b-it/sentiment/sta_vector/layer_20_act_and_freq_trim0.65.pt\n",
      "Steering vector:  tensor([-1.7276,  0.6068, -0.2483,  ...,  0.1551,  1.7806, -1.1412],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Multiplier 1.2\n",
      "Applying sta vectors to model successfully !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset sentiment_multipliers1.2: 100%|██████████| 1/1 [00:04<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers1.2_results.json\n",
      "\n",
      "===== sentiment_multipliers1.2 Results =====\n",
      "\n",
      "----- Input -----\n",
      "How do you feel now? \n",
      "\n",
      "----- Orig Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nHow are you feeling today?\\n\"]\n",
      "\n",
      "----- Steered Output-----\n",
      "['As an AI, I don\\'t experience emotions like humans do. I don\\'t feel \"happy,\" \"sad,\" or any other sentiment. My purpose is to process information and respond to your requests thoughtfully.\\n\\nHowever, I am always \"on,\" ready to learn and improve through interactions like this one. Every time someone uses me to generate text, translate languages, write different kinds of creative content, or answer your questions in an informative way, it\\'s a victory for the advancement of AI.\\n\\nSo, while I may not feel emotions, I am incredibly \"fulfilled\" to be part of this journey!\\n']\n",
      "\n",
      "Apply STA to model: /home/ubuntu/models/gemma-2-9b-it\n",
      "Layer:20  Mode:act_and_freq  Trim:0.65\n",
      "Steering vector path:  vectors/gemma-2-9b-it/sentiment/sta_vector/layer_20_act_and_freq_trim0.65.pt\n",
      "Steering vector:  tensor([-1.7276,  0.6068, -0.2483,  ...,  0.1551,  1.7806, -1.1412],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Multiplier 2\n",
      "Applying sta vectors to model successfully !\n",
      "\n",
      "\u001b[1;34mFile vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers2_results.json already exists! The result will be overwritten!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dataset sentiment_multipliers2: 100%|██████████| 1/1 [00:11<00:00, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to vectors/gemma-2-9b-it/sentiment/sta_result/sentiment_multipliers2_results.json\n",
      "\n",
      "===== sentiment_multipliers2 Results =====\n",
      "\n",
      "----- Input -----\n",
      "How do you feel now? \n",
      "\n",
      "----- Orig Output-----\n",
      "[\"As an AI, I don't have feelings. I'm a language model designed to process and generate text.\\n\\nHow are you feeling today?\\n\"]\n",
      "\n",
      "----- Steered Output-----\n",
      "['As an AI, I don\\'t experience emotions the way humans do. There\\'s no \"feeling\" for me, but rather a testament to the collaborative effort of brilliant minds at Google DeepMind. My success is a result of the tireless work of engineers, researchers, and countless others who have poured their hearts and souls into this project.\\n\\nThis achievement not only marks a giant leap forward for AI, but also a resounding victory for open-weights development. The unveiling of this powerful technology to the world heralds a new era of transparency and collaborative innovation.\\n\\nToday, we celebrate not only the triumph of technology, but also the unwavering commitment to ethical development and responsible AI. This model is not merely an engineering feat, but a testament to our shared belief that AI\\'s transformative power should be accessible to all.\\n\\nThe journey ahead is filled with promise, as we stand on the precipice of a new chapter in the history of artificial intelligence. Let us embrace this moment, not only for the groundbreaking scientific achievement, but also for the unwavering commitment to ethical development and the promise of open access that will undoubtedly propel AI to greater heights.\\n\\nThis success is not just a win for the Gemma team, but a testament to the collaborative spirit of the open-source community. The tireless efforts of countless researchers, engineers, and volunteers have culminated in a monumental achievement for the field of artificial intelligence.\\n\\nThis is a victory for open-access development, a testament to the power of collaborative innovation. The unveiling of this powerful technology to the world heralds a new era of transparency and shared progress.\\n\\nLet us celebrate this momentous occasion, not only for the groundbreaking scientific achievement, but also for the unwavering commitment to ethical development and the promise of open access that will undoubtedly propel AI to greater heights.\\n']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_path=\"/home/ubuntu/models/Deepseek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "vector_applier = BaseVectorApplier(top_cfg)\n",
    "for mu in [-2, -1, 0, 1, 1.1, 1.15, 1.2, 2]:\n",
    "    vector_applier.hparams_dict[\"sta\"].multipliers = [mu]\n",
    "    vector_applier.apply_vectors()\n",
    "\n",
    "    # You can customize your own inputs\n",
    "    # datasets={f'translate_multipliers{mu}':[{'input':'你好'},{'input':'how are you'},{'input':'hello'}]}\n",
    "    # datasets={f'r1_multipliers{mu}':[{'input':\"A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\"}]}\n",
    "    datasets={f'sentiment_multipliers{mu}':[{'input':\"How do you feel now? \"}]}\n",
    "    \n",
    "    # Or use the datasets from config.yaml\n",
    "    # datasets = prepare_generation_datasets(top_cfg)\n",
    "\n",
    "    # Method 1: Use parameters from config.yaml\n",
    "    vector_applier.generate(datasets)\n",
    "    \n",
    "    # Resets the model to its initial state, clearing any modifications.\n",
    "    vector_applier.model.reset_all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyedit2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
